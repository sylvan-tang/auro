# 议题
> why not use custom tokenizer for elasticsearch

## 对比

#### 达到的目的相同
使用分词插件可以将插件打包上传到集群，然后在索引的 mapping properties 中给需要分词的 field 指定对应的 tokenizer，
在给索引新增文档的时候，es 集群就会自动找到设定的 tokenizer 对 field 进行分词，这样做对应的 field 就直接被处理成
分词后的数组；
同时，在进行搜索的时候，es 也会自动根据搜索的 field 对传入的搜索关键词进行分词，然后进行匹配搜索

不使用分词插件的话，需要在新增文档前，对需要进行分词的 field 进行预处理；同时在搜索的时候也得对搜索关键词进行分词，
这样如果在分词方法都统一的情况下，达到的目的都是相同的；

如果在对搜索关键词进行分词的时候，不想和具体的 field 进行分词方法的绑定的话，基本建议是不使用分词插件。

#### 分词插件的部署问题
虽然说插件的使用非常方便，但是部署很麻烦，如果要使用一个新插件，要确保在集群的所有节点上都上传成功，不然会引发报错；
其次，插件上传之后需要重启集群才能使插件生效，这点对集群维护非常不友好；
最后，插件的定义需要符合 es plugin 的定义模式，具体参考 [ik-analyzer](https://github.com/wks/ik-analyzer) 
代码仓库来看的话，有一定的学习成本，如果后续需要自定义分词器，这是一个很大的障碍

#### 集群压力相关
对于大多数存储来说，写入是比读昂贵的操作；对于 es，由于要实现读写分离比较困难，并没有像 mysql 一样的数据库中间件
（如mycat）可以比较方便的实现读写分离，而且还能保持比较低的延迟
所以要保持 es 在线集群的高性能，总的思想还是从减少写入过程对集群的性能损耗入手，如果使用 es 插件，必然会增加写入成本


## 总结
综上所述：使用分词预处理的优势更明显
1. 数据源分词和搜索分词可以分开定义，增加搜索的灵活性，后续可以支持搜索词重写等特性
2. 部署更友好
3. 减少集群写入压力